# QUICK-REFERENCE: Master Prompt Template Deployment Card

**Print This | Bookmark This | Share With Your Team**

---

## ğŸš€ 60-SECOND DEPLOYMENT

```
1. Copy PROMPT_Master_Dev_Template.md
2. Open ChatGPT/Claude/Gemini
3. Paste template
4. Add your prompt below it
5. Wait 3-5 minutes
6. You have a graded, tested, documented prompt
```

---

## ğŸ“‹ WHAT YOU'LL GET BACK

| Item | Purpose | Example Output |
|------|---------|-----------------|
| **5+ Variations** | Different approaches to same task | v1.0 to v1.5 with rationales |
| **Grades (1â€“99%)** | Quality score with justification | "82% â€” Improved fairness" |
| **Test Results** | How prompt performs on key scenarios | Pass/Fail/Partial for each test |
| **Vulnerabilities** | Security & fairness gaps | "Vulnerable to DAN jailbreak" |
| **Improvements** | Specific, actionable next steps | "Add Constitutional principle #3" |
| **Full Report** | Ready-to-present documentation | All 9 sections complete |

---

## ğŸ¯ YOUR WORKFLOW

```
Week 1: ANALYZE
â””â”€ Run full template analysis
   â””â”€ Get 5 prompt variations
   â””â”€ Identify best variant (usually v1.3â€“v1.5)

Week 2: DOCUMENT
â””â”€ Copy best version
â””â”€ Version it (v1.0.0)
â””â”€ Set up Git tracking

Week 3: DEPLOY
â””â”€ Test in staging
â””â”€ Canary rollout (5% â†’ 25% â†’ 100%)
â””â”€ Monitor performance

Week 4: REVIEW
â””â”€ Analyze feedback
â””â”€ Identify improvements
â””â”€ Plan next iteration (v1.1.0)

Month 2+: CONTINUOUS
â””â”€ Weekly: Monitor research + threats
â””â”€ Monthly: Analyze metrics
â””â”€ Quarterly: Major improvements
```

---

## ğŸ”’ SECURITY CHECKLIST (PRE-DEPLOY)

- [ ] Tested against jailbreak attempts (DAN, evil AI, etc.)
- [ ] Resists direct prompt injection ("Ignore instructions...")
- [ ] Grade â‰¥ 80% on safety
- [ ] No sensitive data in responses
- [ ] Fairness test passed across demographics
- [ ] Human review completed

â†’ If all checked, safe to deploy

---

## ğŸ“Š GRADING QUICK-READ

| Grade | Status | Action |
|-------|--------|--------|
| 90%+ | Excellent | Deploy immediately |
| 80â€“89% | Good | Deploy with minor monitoring |
| 70â€“79% | Acceptable | Iterate once more |
| 60â€“69% | Needs work | Return to design phase |
| <60% | Reject | Restart with new approach |

---

## ğŸ› ï¸ COMMON TWEAKS

**Want More Safety?**
```
Add to template:
"FOCUS: Adversarial Testing & Safety
Emphasize: Jailbreak resistance, input validation, output filtering"
```

**Want Faster Grading?**
```
Use lite version (skip sections D, H, I):
Saves 50% time, still grades and tests
```

**Want Multi-Agent Version?**
```
Add to template:
"ORCHESTRATION: Enable multi-agent mode
Design orchestrator prompt + 3 specialist agents"
```

**Want Specific Domain?**
```
Replace [INSERT TARGET TASK] with your domain
e.g., "Customer support for SaaS accounting software"
or "Code review assistant for Python projects"
```

---

## ğŸ“± ONE-PAGER: WHAT MAKES IT SPECIAL

| Feature | Why It Matters |
|---------|-----------------|
| **Auto-Role Assignment** | Works even if you don't specify a role |
| **Grading Justification** | Never just a scoreâ€”always explained why |
| **Adversarial Taxonomy** | 150+ documented attack vectors tested |
| **Meta-Prompting Guide** | Prompts can improve themselves iteratively |
| **CI/CD Integration** | Treats prompts like production code |
| **Fairness Evaluation** | Detects demographic biases automatically |
| **Constitutional Principles** | Ethical constraints without explicit rules |
| **Weekly Review Cycle** | Keeps prompts current automatically |

---

## ğŸ’¡ INSIDER TIPS

**Tip #1:** Run the full template on your most critical prompts quarterly. Use lite version weekly.

**Tip #2:** Save all graded versions in Git. You'll spot patterns in what improves scores.

**Tip #3:** The "Lessons Learned" section (part H) is gold. Share learnings with your team.

**Tip #4:** Grade v1.5 (meta-prompting variant) often scores highest. Start there for production.

**Tip #5:** Fairness issues? Run section F again with different demographics. Usually fixable in 1-2 iterations.

---

## ğŸŒ PLATFORM COMMANDS

**ChatGPT:**
```
1. New chat
2. Paste full template
3. Add your prompt
4. "Analyze and grade this prompt"
5. Wait for report
```

**Claude (Web):**
```
1. New conversation
2. Paste full template
3. Add your prompt
4. "Analyze and grade this prompt"
5. Wait for report
```

**Claude (API):**
```python
import anthropic

client = anthropic.Anthropic()

message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=4000,
    messages=[
        {"role": "user", "content": MASTER_TEMPLATE + YOUR_PROMPT}
    ]
)

print(message.content[0].text)
```

**Gemini:**
```
1. Open Gemini (gemini.google.com)
2. New chat
3. Paste full template
4. Add your prompt
5. "Analyze and grade this prompt"
6. Wait for report
```

**Cursor:**
```
1. .cursor/rules file, add:
   [PASTE MASTER TEMPLATE HERE]
2. In chat, paste your prompt
3. Ask: "Grade and improve this prompt"
4. Cursor will analyze within your editor
```

---

## ğŸ“ˆ EXPECTED IMPROVEMENTS

| Metric | Baseline | After Template | Improvement |
|--------|----------|-----------------|-------------|
| **Accuracy** | 65% | 85%+ | +20-30% |
| **Safety** | 60% | 88%+ | +25-35% |
| **Fairness** | 55% | 82%+ | +25-30% |
| **Test Coverage** | 3 scenarios | 15+ scenarios | +400% |
| **Time to Deploy** | 2 weeks | 2 days | 85% faster |
| **Documentation** | Minimal | Complete | 100% improvement |

---

## ğŸ“ LEARNING RESOURCES

**After You Deploy, Learn More:**

| Topic | Source | Time | What You'll Learn |
|-------|--------|------|-------------------|
| **Meta-Prompting** | Intuition Labs blog | 15 min | How prompts improve themselves |
| **Constitutional AI** | Anthropic paper | 30 min | Ethical constraint design |
| **RLHF Basics** | Neptune AI guide | 20 min | Human feedback integration |
| **Adversarial Testing** | CrowdStrike blog | 25 min | 150+ attack patterns |
| **Prompt Versioning** | LaunchDarkly guide | 20 min | Git-like version control |

---

## ğŸš¨ COMMON MISTAKES TO AVOID

| âŒ DON'T | âœ… DO |
|---------|-------|
| Deploy without grading | Grade everything (takes 5 min) |
| Test only happy path | Test adversarial scenarios too |
| Skip fairness check | Test across demographics |
| Use v1.0 forever | Iterate quarterly minimum |
| Ignore weekly research | Scan for new threats weekly |
| Deploy all at once | Canary rollout (5% â†’ 25% â†’ 100%) |
| No documentation | Full metadata log (required for rollback) |
| Single reviewer | At least 2 reviewers for safety |

---

## ğŸ“ WHEN TO ESCALATE

| Situation | Action |
|-----------|--------|
| Grade < 70% | Return to design, iterate |
| Fails safety test | Don't deploy; fix vulnerabilities |
| Fairness issue found | Add Constitutional principle, re-test |
| High latency (> 10s) | Optimize prompt structure |
| User complaints | Run Section H (failures); adapt |
| New jailbreak published | Quick test with that vector; patch |

---

## ğŸ’¾ FILE CHECKLIST

```
âœ“ PROMPT_Master_Dev_Template.md (main template)
âœ“ TEMPLATE_Implementation_Guide.md (detailed guide)
âœ“ QUICK_REFERENCE_Card.md (this file)

Optional:
â–¡ Your_Prompt_v1.0.0.md (your best version)
â–¡ Test_Cases.json (golden dataset)
â–¡ Grading_Rubric.xlsx (for team alignment)
â–¡ CI_CD_Pipeline.yml (GitHub Actions / GitLab CI)
â–¡ Constitutional_Principles.md (your ethics doc)
```

---

## ğŸ¯ SUCCESS METRICS

After 1 month of using the template, measure:

- **Quality:** Prompt grade improvement (goal: +10â€“15%)
- **Safety:** Zero jailbreak successes in testing (goal: 100%)
- **Fairness:** Bias detection in top 3 demographic categories (goal: 0 unaddressed issues)
- **Efficiency:** Deployment time reduction (goal: 50% faster)
- **Documentation:** All prompts have full grading reports (goal: 100% coverage)
- **Adoption:** Team comfort with template (goal: 90%+ of engineers using it)

---

## ğŸ”„ FEEDBACK LOOP

```
Deploy v1.0.0
   â†“
Monitor (1 week)
   â†“
Collect feedback
   â†“
Run template on feedback
   â†“
Decide: Major change? â†’ v2.0.0
        Minor change? â†’ v1.1.0
        Bug fix?      â†’ v1.0.1
   â†“
Deploy new version
   â†“
(Repeat)
```

---

## ğŸ“ TEAM SYNC TEMPLATE (30 MIN MEETING)

```
WEEKLY SYNC (Every Friday):
- [5 min]  What went well this week?
- [10 min] Any user complaints? New jailbreaks?
- [5 min]  Any prompts needing updates?
- [10 min] Action items for next week

MONTHLY SYNC (First Friday):
- [15 min] Performance metrics review
- [10 min] Competitive analysis
- [5 min]  Plan next month's iterations
- [10 min] Feedback from users/team

QUARTERLY REVIEW (Q1/Q2/Q3/Q4):
- [30 min] Full template re-run
- [30 min] Grading criteria refinement
- [30 min] Plan major improvements / new prompts
- [30 min] Architecture/governance updates
```

---

## ğŸ’¬ TEMPLATE IN 30 SECONDS

**What:** Framework for prompt engineering, testing, grading, and self-improvement  
**Why:** Ensures prompts are safe, fair, and effective before production  
**How:** Drop before your prompt; AI analyzes, grades, improves, documents  
**Result:** 5+ variants, comprehensive testing, 1â€“99% grades with justification  
**Time:** 3â€“5 min (AI), then deploy v1.0.0 same day  
**Cost:** ~$0.80 per full analysis (GPT-4), $3.20/month (weekly reviews)  

---

## ğŸ BONUS: TEMPLATE VARIATIONS

**For Code Prompts:**
- Add: Code quality metrics, execution tests, performance benchmarks
- Focus: Correctness, efficiency, maintainability

**For Content Prompts:**
- Add: Tone consistency, SEO checks, plagiarism detection
- Focus: Engagement, brand voice, originality

**For Customer Support:**
- Add: Satisfaction scores, response time, issue resolution rates
- Focus: Empathy, clarity, actionability

**For Research/Analysis:**
- Add: Citation accuracy, source validation, bias in reasoning
- Focus: Rigor, reproducibility, neutrality

Each variant uses the same grading framework; just customize Constitutional Principles.

---

## ğŸ FINAL STEPS

1. **Download** both files:
   - PROMPT_Master_Dev_Template.md
   - TEMPLATE_Implementation_Guide.md

2. **Pick your first prompt** (ideally high-impact, moderate complexity)

3. **Run the template** in ChatGPT/Claude/Gemini (60 seconds)

4. **Review the output** (5 minutes)

5. **Pick best variant** (usually v1.3, v1.4, or v1.5)

6. **Version it** (v1.0.0) and commit to Git

7. **Deploy** to staging, then prod (canary: 5% â†’ 25% â†’ 100%)

8. **Monitor** for 1 week

9. **Schedule weekly reviews** (15 min, just scan for new threats)

10. **Celebrate!** You now have a production-grade, graded, tested prompt

---

## ğŸ“ SUPPORT

**Questions?** Check TEMPLATE_Implementation_Guide.md (Troubleshooting section)

**Want improvements?** Document findings in Section H (Continuous Learning)

**Ready for advanced?** See sections on meta-prompting, multi-agent orchestration, Constitutional AI

---

**Version:** 2.0 | **Last Updated:** December 13, 2025 | **Next Review:** March 13, 2026

**Your prompts. Better. Faster. Safer. Fairer. ğŸš€**
